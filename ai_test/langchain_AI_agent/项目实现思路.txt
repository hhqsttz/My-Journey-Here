å¤§æ¨¡å‹é¡¹ç›®ï¼ˆåˆ«å¯¼å˜é‡ï¼Œå¯¼æ¥å£ï¼Œä¸ç„¶ç”µè„‘ä¼šç‚¸ï¼‰
è¦å¯¼çš„åŒ…
æ¨¡å‹åº“ï¼š
huggingface_hub
modelscope
å‰ç«¯/åç«¯æ¡†æ¶:
gradio
fastapi
æ¨¡å‹å®ä¾‹åˆ›å»ºç›¸å…³ï¼š
openai
langchain_openai
langchain_huggingface
langchainæ¡†æ¶ï¼š
langchain
langchain-community
transformæ¨ç†æ¡†æ¶ï¼š
stancesâ€”transform


ä¸€.ç¬¬ä¸€æ­¥ï¼Œä¸‹è½½æ¨¡å‹
 pip install -U huggingface_hub
 é…ç½®ç¯å¢ƒå˜é‡
 HF_ENDPOINT = https://hf_mirror.com
from ï¼ˆhuggingface_hub/modelscopeï¼‰ import snapshot_download
# ==== é…ç½®åŒºåŸŸ ====
# Hugging Face æ¨¡å‹ä»“åº“ ID (åœ¨æ¨¡å‹é¡µé¢ URL å¯ä»¥æ‰¾åˆ°)
repo_id = "Qwen/Qwen2.5-Omni-3B"
# æ¨¡å‹ä¸‹è½½ä¿å­˜ç›®å½• (å¯ä»¥æ¢æˆè‡ªå·±çš„è·¯å¾„)
cache_dir = "/root/autodl-tmp/models"
# æŒ‡å®šç‰ˆæœ¬ (huggingfaceæ˜¯ "main"ï¼Œmodelscopeæ˜¯"master")
revision = "main"/"master"
# =================
# ä¸‹è½½æ•´ä¸ªä»“åº“
model_dir = snapshot_download(
    repo_id=repo_id,
    cache_dir=cache_dir,
    revision=revision
)
print(f"âœ… æ¨¡å‹å·²å®Œæ•´ä¸‹è½½åˆ°: {model_dir}")
äºŒ.é€šè¿‡éƒ¨ç½²æ¡†æ¶éƒ¨ç½²
ä»¥vllmä¸ºä¾‹
-- deepseek-r1-0528-qwen3-8B
  python -m vllm.entrypoints.openai.api_server \
  --model /root/autodl-tmp/models/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
  --served-model-name ds-qwen3-8b \
  --max-model-len 8k \
  --host 0.0.0.0 \
  --port 6006 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes
é€šç”¨
python -m vllm.entrypoints.openai.api_server \
  --model æœ¬åœ°æ¨¡å‹è·¯å¾„\
  --served-model-name æ¨¡å‹åå­—\
  --max-model-len ä¸Šä¸‹æ–‡é•¿åº¦ \
  --host 0.0.0.0 \
  --port 6006 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes
ä¸‰.é¡¹ç›®åˆ›å»º
1.å¤šæ¨¡æ€aiæ™ºèƒ½åŠ©æ‰‹
 å¤§æ¨¡å‹å®ä¾‹åˆ›å»ºæ¨¡å—
 .envæ¥å­˜å¤§æ¨¡å‹çš„api_keyå’Œbase_urlé€šè¿‡env_test.pyå°†envå¯¼å…¥åˆ°è®¾ç½®çš„å˜é‡ä¸­ï¼Œllm_model.pyæ¥åˆ›å»ºllm

æ¨¡å—å‡½æ•°ä»‹ç»


# ğŸ¯ SentenceTransformer.encode() æ–¹æ³•å‚æ•°è¯¦è§£

## ğŸ“‹ å‚æ•°æ¦‚è¿°

```python
def encode(
    sentences: str | list[str] | ndarray,           # è¦ç¼–ç çš„æ–‡æœ¬
    prompt_name: str | None = None,                 # é¢„å®šä¹‰æç¤ºæ¨¡æ¿åç§°
    prompt: str | None = None,                      # è‡ªå®šä¹‰æç¤ºæ–‡æœ¬
    batch_size: int = 32,                           # æ‰¹å¤„ç†å¤§å°
    show_progress_bar: bool | None = None,          # æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
    output_value: Literal["sentence_embedding", "token_embeddings"] | None = "sentence_embedding",  # è¾“å‡ºç±»å‹
    precision: Literal["float32", "int8", "uint8", "binary", "ubinary"] = "float32",  # ç²¾åº¦æ ¼å¼
    convert_to_numpy: bool = True,                  # è½¬æ¢ä¸ºnumpyæ•°ç»„
    convert_to_tensor: bool = False,                # è½¬æ¢ä¸ºPyTorchå¼ é‡
    device: str | list[str | device] | None = None, # è¾“å‡ºè®¾å¤‡
    normalize_embeddings: bool = False,             # æ˜¯å¦æ ‡å‡†åŒ–åµŒå…¥
    truncate_dim: int | None = None,                # æˆªæ–­ç»´åº¦
    pool: dict[Literal["input", "output", "processes"], Any] | None = None,  # å¤šè¿›ç¨‹æ± 
    chunk_size: int | None = None,                  # åˆ†å—å¤§å°
    **kwargs: Any                                   # å…¶ä»–å‚æ•°
) -> list[Tensor] | ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]:
```

## ğŸª æ ¸å¿ƒå‚æ•°è¯¦è§£

### 1. `sentences` - è¾“å…¥æ–‡æœ¬
```python
# å•å¥æ–‡æœ¬
embeddings = model.encode("è¿™æ˜¯ä¸€ä¸ªå¥å­")

# å¤šå¥æ–‡æœ¬åˆ—è¡¨
embeddings = model.encode(["å¥å­1", "å¥å­2", "å¥å­3"])

# numpyæ•°ç»„ï¼ˆåŒ…å«å­—ç¬¦ä¸²ï¼‰
sentences_array = np.array(["æ–‡æœ¬1", "æ–‡æœ¬2"])
embeddings = model.encode(sentences_array)
```

### 2. `batch_size` - æ‰¹å¤„ç†å¤§å°
```python
# å°æ‰¹é‡ï¼ˆå†…å­˜è¾ƒå°‘æ—¶ï¼‰
embeddings = model.encode(sentences, batch_size=16)

# å¤§æ‰¹é‡ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
embeddings = model.encode(sentences, batch_size=64)

# è‡ªåŠ¨è°ƒæ•´ï¼ˆæ ¹æ®ç¡¬ä»¶ï¼‰
embeddings = model.encode(sentences, batch_size="auto")
```

### 3. `output_value` - è¾“å‡ºç±»å‹
```python
# é»˜è®¤ï¼šå¥å­åµŒå…¥ï¼ˆæ¨èï¼‰
sentence_embeddings = model.encode(sentences, output_value="sentence_embedding")

# è·å–æ‰€æœ‰tokençš„åµŒå…¥ï¼ˆç”¨äºç‰¹å®šä»»åŠ¡ï¼‰
token_embeddings = model.encode(sentences, output_value="token_embeddings")
# è¿”å›æ ¼å¼ï¼š{"input_ids": [...], "token_embeddings": [...]}
```

### 4. è¾“å‡ºæ ¼å¼æ§åˆ¶
```python
# é»˜è®¤è¿”å›numpyæ•°ç»„ï¼ˆæ¨èï¼‰
embeddings = model.encode(sentences, convert_to_numpy=True)

# è¿”å›PyTorchå¼ é‡
tensor_embeddings = model.encode(sentences, convert_to_tensor=True)

# æŒ‡å®šè¾“å‡ºè®¾å¤‡
gpu_embeddings = model.encode(sentences, device="cuda")
```

### 5. åµŒå…¥åå¤„ç†
```python
# æ ‡å‡†åŒ–åµŒå…¥å‘é‡ï¼ˆå•ä½é•¿åº¦ï¼‰
normalized_embeddings = model.encode(sentences, normalize_embeddings=True)

# æˆªæ–­åˆ°æŒ‡å®šç»´åº¦
truncated_embeddings = model.encode(sentences, truncate_dim=256)
```

### 6. è¿›åº¦å’Œæ€§èƒ½æ§åˆ¶
```python
# æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆå¤„ç†å¤§é‡æ–‡æœ¬æ—¶ï¼‰
embeddings = model.encode(sentences, show_progress_bar=True)

# ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
embeddings = model.encode(sentences, pool={"processes": 4})

# åˆ†å—å¤„ç†è¶…å¤§æ–‡æœ¬
embeddings = model.encode(sentences, chunk_size=1000)
```

## ğŸš€ å®ç”¨ç¤ºä¾‹ç»„åˆ

### ç¤ºä¾‹1ï¼šåŸºç¡€ç”¨æ³•
```python
# æœ€ç®€å•çš„ç”¨æ³•
sentences = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£", "æœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜ä¸–ç•Œ"]
embeddings = model.encode(sentences)

print(f"åµŒå…¥å½¢çŠ¶: {embeddings.shape}")  # (3, 1024)
```

### ç¤ºä¾‹2ï¼šé«˜æ€§èƒ½å¤„ç†
```python
# å¤„ç†å¤§é‡æ–‡æœ¬çš„ä¼˜åŒ–é…ç½®
large_corpus = ["æ–‡æœ¬{}".format(i) for i in range(1000)]

embeddings = model.encode(
    large_corpus,
    batch_size=64,              # åˆé€‚çš„æ‰¹å¤„ç†å¤§å°
    show_progress_bar=True,     # æ˜¾ç¤ºè¿›åº¦
    normalize_embeddings=True,  # æ ‡å‡†åŒ–ä¾¿äºç›¸ä¼¼åº¦è®¡ç®—
    device="cuda" if torch.cuda.is_available() else "cpu"
)
```

### ç¤ºä¾‹3ï¼šé«˜çº§åŠŸèƒ½
```python
# è·å–tokençº§åµŒå…¥ç”¨äºç‰¹å®šåˆ†æ
text = "è‡ªç„¶è¯­è¨€å¤„ç†å¾ˆæœ‰è¶£"
result = model.encode(
    text,
    output_value="token_embeddings",  # è·å–tokenåµŒå…¥
    convert_to_tensor=True           # è¿”å›å¼ é‡
)

print("Token IDs:", result["input_ids"])
print("TokenåµŒå…¥å½¢çŠ¶:", result["token_embeddings"].shape)
```

### ç¤ºä¾‹4ï¼šå†…å­˜ä¼˜åŒ–
```python
# å†…å­˜å—é™ç¯å¢ƒä¸‹çš„é…ç½®
embeddings = model.encode(
    sentences,
    batch_size=8,                   # å°æ‰¹é‡å‡å°‘å†…å­˜ä½¿ç”¨
    precision="int8",               # ä½¿ç”¨8ä½æ•´æ•°èŠ‚çœç©ºé—´
    truncate_dim=512,               # æˆªæ–­åˆ°è¾ƒå°ç»´åº¦
    convert_to_numpy=True           # ä½¿ç”¨numpyè€ŒéPyTorch
)
```

## ğŸ“Š å‚æ•°é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èå‚æ•°é…ç½® |
| :--- | :--- |
| **å¸¸è§„ä½¿ç”¨** | `batch_size=32, convert_to_numpy=True` |
| **ç›¸ä¼¼åº¦è®¡ç®—** | `normalize_embeddings=True` |
| **å¤§è§„æ¨¡å¤„ç†** | `batch_size=64, show_progress_bar=True, pool={"processes": 4}` |
| **å†…å­˜ä¼˜åŒ–** | `batch_size=16, precision="int8", truncate_dim=256` |
| **GPUåŠ é€Ÿ** | `device="cuda", convert_to_tensor=True` |
| **è¯¦ç»†åˆ†æ** | `output_value="token_embeddings"` |

## âš ï¸ æ³¨æ„äº‹é¡¹

1.  **å†…å­˜ç®¡ç†**ï¼šå¤§æ‰¹é‡å¤„ç†ä¼šå ç”¨æ›´å¤šå†…å­˜ï¼Œè¯·æ ¹æ®ç¡¬ä»¶è°ƒæ•´ `batch_size`
2.  **ç²¾åº¦æƒè¡¡**ï¼šé™ä½ç²¾åº¦ï¼ˆå¦‚ `int8`ï¼‰å¯ä»¥èŠ‚çœç©ºé—´ä½†å¯èƒ½æŸå¤±ä¸€äº›ç²¾åº¦
3.  **æ ‡å‡†åŒ–æ—¶æœº**ï¼šå¦‚æœåç»­è¦è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå»ºè®®è®¾ç½® `normalize_embeddings=True`
4.  **è®¾å¤‡ä¸€è‡´æ€§**ï¼šç¡®ä¿è¾“å…¥è®¾å¤‡å’Œæ¨¡å‹è®¾å¤‡ä¸€è‡´ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“

è¿™äº›å‚æ•°è®©ä½ èƒ½å¤Ÿç²¾ç»†æ§åˆ¶åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œç¡¬ä»¶ç¯å¢ƒã€‚æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‚æ•°ç»„åˆï¼