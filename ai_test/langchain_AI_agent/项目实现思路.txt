大模型项目（别导变量，导接口，不然电脑会炸）
要导的包
模型库：
huggingface_hub
modelscope
前端/后端框架:
gradio
fastapi
模型实例创建相关：
openai
langchain_openai
langchain_huggingface
langchain框架：
langchain
langchain-community
transform推理框架：
stances—transform


一.第一步，下载模型
 pip install -U huggingface_hub
 配置环境变量
 HF_ENDPOINT = https://hf_mirror.com
from （huggingface_hub/modelscope） import snapshot_download
# ==== 配置区域 ====
# Hugging Face 模型仓库 ID (在模型页面 URL 可以找到)
repo_id = "Qwen/Qwen2.5-Omni-3B"
# 模型下载保存目录 (可以换成自己的路径)
cache_dir = "/root/autodl-tmp/models"
# 指定版本 (huggingface是 "main"，modelscope是"master")
revision = "main"/"master"
# =================
# 下载整个仓库
model_dir = snapshot_download(
    repo_id=repo_id,
    cache_dir=cache_dir,
    revision=revision
)
print(f"✅ 模型已完整下载到: {model_dir}")
二.通过部署框架部署
以vllm为例
-- deepseek-r1-0528-qwen3-8B
  python -m vllm.entrypoints.openai.api_server \
  --model /root/autodl-tmp/models/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B \
  --served-model-name ds-qwen3-8b \
  --max-model-len 8k \
  --host 0.0.0.0 \
  --port 6006 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes
通用
python -m vllm.entrypoints.openai.api_server \
  --model 本地模型路径\
  --served-model-name 模型名字\
  --max-model-len 上下文长度 \
  --host 0.0.0.0 \
  --port 6006 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes
三.项目创建
1.多模态ai智能助手
 大模型实例创建模块
 .env来存大模型的api_key和base_url通过env_test.py将env导入到设置的变量中，llm_model.py来创建llm

模块函数介绍


# 🎯 SentenceTransformer.encode() 方法参数详解

## 📋 参数概述

```python
def encode(
    sentences: str | list[str] | ndarray,           # 要编码的文本
    prompt_name: str | None = None,                 # 预定义提示模板名称
    prompt: str | None = None,                      # 自定义提示文本
    batch_size: int = 32,                           # 批处理大小
    show_progress_bar: bool | None = None,          # 是否显示进度条
    output_value: Literal["sentence_embedding", "token_embeddings"] | None = "sentence_embedding",  # 输出类型
    precision: Literal["float32", "int8", "uint8", "binary", "ubinary"] = "float32",  # 精度格式
    convert_to_numpy: bool = True,                  # 转换为numpy数组
    convert_to_tensor: bool = False,                # 转换为PyTorch张量
    device: str | list[str | device] | None = None, # 输出设备
    normalize_embeddings: bool = False,             # 是否标准化嵌入
    truncate_dim: int | None = None,                # 截断维度
    pool: dict[Literal["input", "output", "processes"], Any] | None = None,  # 多进程池
    chunk_size: int | None = None,                  # 分块大小
    **kwargs: Any                                   # 其他参数
) -> list[Tensor] | ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]:
```

## 🎪 核心参数详解

### 1. `sentences` - 输入文本
```python
# 单句文本
embeddings = model.encode("这是一个句子")

# 多句文本列表
embeddings = model.encode(["句子1", "句子2", "句子3"])

# numpy数组（包含字符串）
sentences_array = np.array(["文本1", "文本2"])
embeddings = model.encode(sentences_array)
```

### 2. `batch_size` - 批处理大小
```python
# 小批量（内存较少时）
embeddings = model.encode(sentences, batch_size=16)

# 大批量（性能优化）
embeddings = model.encode(sentences, batch_size=64)

# 自动调整（根据硬件）
embeddings = model.encode(sentences, batch_size="auto")
```

### 3. `output_value` - 输出类型
```python
# 默认：句子嵌入（推荐）
sentence_embeddings = model.encode(sentences, output_value="sentence_embedding")

# 获取所有token的嵌入（用于特定任务）
token_embeddings = model.encode(sentences, output_value="token_embeddings")
# 返回格式：{"input_ids": [...], "token_embeddings": [...]}
```

### 4. 输出格式控制
```python
# 默认返回numpy数组（推荐）
embeddings = model.encode(sentences, convert_to_numpy=True)

# 返回PyTorch张量
tensor_embeddings = model.encode(sentences, convert_to_tensor=True)

# 指定输出设备
gpu_embeddings = model.encode(sentences, device="cuda")
```

### 5. 嵌入后处理
```python
# 标准化嵌入向量（单位长度）
normalized_embeddings = model.encode(sentences, normalize_embeddings=True)

# 截断到指定维度
truncated_embeddings = model.encode(sentences, truncate_dim=256)
```

### 6. 进度和性能控制
```python
# 显示进度条（处理大量文本时）
embeddings = model.encode(sentences, show_progress_bar=True)

# 使用多进程加速
embeddings = model.encode(sentences, pool={"processes": 4})

# 分块处理超大文本
embeddings = model.encode(sentences, chunk_size=1000)
```

## 🚀 实用示例组合

### 示例1：基础用法
```python
# 最简单的用法
sentences = ["今天天气真好", "人工智能很有趣", "机器学习正在改变世界"]
embeddings = model.encode(sentences)

print(f"嵌入形状: {embeddings.shape}")  # (3, 1024)
```

### 示例2：高性能处理
```python
# 处理大量文本的优化配置
large_corpus = ["文本{}".format(i) for i in range(1000)]

embeddings = model.encode(
    large_corpus,
    batch_size=64,              # 合适的批处理大小
    show_progress_bar=True,     # 显示进度
    normalize_embeddings=True,  # 标准化便于相似度计算
    device="cuda" if torch.cuda.is_available() else "cpu"
)
```

### 示例3：高级功能
```python
# 获取token级嵌入用于特定分析
text = "自然语言处理很有趣"
result = model.encode(
    text,
    output_value="token_embeddings",  # 获取token嵌入
    convert_to_tensor=True           # 返回张量
)

print("Token IDs:", result["input_ids"])
print("Token嵌入形状:", result["token_embeddings"].shape)
```

### 示例4：内存优化
```python
# 内存受限环境下的配置
embeddings = model.encode(
    sentences,
    batch_size=8,                   # 小批量减少内存使用
    precision="int8",               # 使用8位整数节省空间
    truncate_dim=512,               # 截断到较小维度
    convert_to_numpy=True           # 使用numpy而非PyTorch
)
```

## 📊 参数选择指南

| 场景 | 推荐参数配置 |
| :--- | :--- |
| **常规使用** | `batch_size=32, convert_to_numpy=True` |
| **相似度计算** | `normalize_embeddings=True` |
| **大规模处理** | `batch_size=64, show_progress_bar=True, pool={"processes": 4}` |
| **内存优化** | `batch_size=16, precision="int8", truncate_dim=256` |
| **GPU加速** | `device="cuda", convert_to_tensor=True` |
| **详细分析** | `output_value="token_embeddings"` |

## ⚠️ 注意事项

1.  **内存管理**：大批量处理会占用更多内存，请根据硬件调整 `batch_size`
2.  **精度权衡**：降低精度（如 `int8`）可以节省空间但可能损失一些精度
3.  **标准化时机**：如果后续要进行余弦相似度计算，建议设置 `normalize_embeddings=True`
4.  **设备一致性**：确保输入设备和模型设备一致，避免不必要的数据传输

这些参数让你能够精细控制嵌入生成过程，适应不同的应用场景和硬件环境。根据你的具体需求选择合适的参数组合！