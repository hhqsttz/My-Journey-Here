import asyncio
import json
from typing import Dict, Any, List
import gradio as gr
from gradio import ChatMessage
from langchain_core.messages import ToolMessage, AIMessage, ToolCall, HumanMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import MemorySaver
from langgraph.constants import END, START
from langgraph.graph import MessagesState, StateGraph
from langgraph.types import interrupt, Command

from agent.env_utils import ZHIPU_API_KEY
from agent.my_llm import llm

# å¤–ç½‘ä¸Šå…¬å¼€ MCP æœåŠ¡ç«¯çš„è¿æ¥é…ç½®
zhipuai_mcp_server_config = {
    'url': 'https://open.bigmodel.cn/api/mcp/web_search/sse?Authorization=' + ZHIPU_API_KEY,
    'transport': 'sse',
}

my12306_mcp_server_config = {
    'url': 'https://mcp.api-inference.modelscope.net/f938a50626714f/sse',
    'transport': 'sse',
}

chart_mcp_server_config = {
    'url': 'https://mcp.api-inference.modelscope.net/c76cefe1dbae4f/sse',
    'transport': 'sse',
}

# MCPçš„å®¢æˆ·ç«¯
mcp_client = MultiServerMCPClient(
    {
        'chart_mcp': chart_mcp_server_config,
        'my12306_mcp': my12306_mcp_server_config,
        'zhipuai_mcp': zhipuai_mcp_server_config,
    }
)


class BasicToolsNode:
    """
    å¼‚æ­¥å·¥å…·èŠ‚ç‚¹ï¼Œç”¨äºå¹¶å‘æ‰§è¡ŒAIMessageä¸­è¯·æ±‚çš„å·¥å…·è°ƒç”¨

    åŠŸèƒ½ï¼š
    1. æ¥æ”¶å·¥å…·åˆ—è¡¨å¹¶å»ºç«‹åç§°ç´¢å¼•
    2. å¹¶å‘æ‰§è¡Œæ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨è¯·æ±‚
    3. è‡ªåŠ¨å¤„ç†åŒæ­¥/å¼‚æ­¥å·¥å…·é€‚é…
    """

    def __init__(self, tools: list):
        """åˆå§‹åŒ–å·¥å…·èŠ‚ç‚¹
        Args:
            tools: å·¥å…·åˆ—è¡¨ï¼Œæ¯ä¸ªå·¥å…·éœ€åŒ…å«nameå±æ€§
        """
        self.tools_by_name = {tool.name: tool for tool in tools}  # æ‰€æœ‰å·¥å…·åå­—çš„å­—å…¸

    async def __call__(self, state: Dict[str, Any]) -> Dict[str, List[ToolMessage]]:
        """å¼‚æ­¥è°ƒç”¨å…¥å£
        Args:
            state: è¾“å…¥å­—å…¸ï¼Œéœ€åŒ…å«"messages"å­—æ®µ
        Returns:
            åŒ…å«ToolMessageåˆ—è¡¨çš„å­—å…¸
        Raises:
            ValueError: å½“è¾“å…¥æ— æ•ˆæ—¶æŠ›å‡º
        """
        # 1. è¾“å…¥éªŒè¯
        if not (messages := state.get("messages")):
            raise ValueError("è¾“å…¥æ•°æ®ä¸­æœªæ‰¾åˆ°æ¶ˆæ¯å†…å®¹")  # æ”¹è¿›åçš„ä¸­æ–‡é”™è¯¯æç¤º
        message: AIMessage = messages[-1]  # å–æœ€æ–°æ¶ˆæ¯: AIMessage

        tool_name = message.tool_calls[0]["name"] if message.tool_calls else None
        if tool_name == 'webSearchStd' or tool_name == 'webSearchSogou':
            response = interrupt(
                f"AIå¤§æ¨¡å‹å°è¯•è°ƒç”¨å·¥å…· `{tool_name}`ï¼Œ\n"
                "è¯·å®¡æ ¸å¹¶é€‰æ‹©ï¼šæ‰¹å‡†ï¼ˆyï¼‰æˆ–ç›´æ¥ç»™æˆ‘å·¥å…·æ‰§è¡Œçš„ç­”æ¡ˆã€‚"
            )
            # response(å­—å…¸): ç”±äººå·¥è¾“å…¥çš„ï¼šæ‰¹å‡†(y),å·¥å…·æ‰§è¡Œçš„ç­”æ¡ˆæˆ–è€…æ‹’ç»æ‰§è¡Œå·¥å…·çš„ç†ç”±
            # æ ¹æ®äººå·¥å“åº”ç±»å‹å¤„ç†
            if response["answer"] == "y":
                pass  # ç›´æ¥ä½¿ç”¨åŸå‚æ•°ç»§ç»­æ‰§è¡Œ
            else:
                return {"messages": [ToolMessage(
                    content=f"äººå·¥ç»ˆæ­¢äº†è¯¥å·¥å…·çš„è°ƒç”¨ï¼Œç»™å‡ºçš„ç†ç”±æˆ–è€…ç­”æ¡ˆæ˜¯:{response['answer']}",
                    name=tool_name,
                    tool_call_id=message.tool_calls[0]['id'],
                )]}

        # 2. å¹¶å‘æ‰§è¡Œå·¥å…·è°ƒç”¨
        outputs = await self._execute_tool_calls(message.tool_calls)
        return {"messages": outputs}

    async def _execute_tool_calls(self, tool_calls: list[Dict]) -> List[ToolMessage]:
        """æ‰§è¡Œå®é™…å·¥å…·è°ƒç”¨
        Args:
            tool_calls: å·¥å…·è°ƒç”¨è¯·æ±‚åˆ—è¡¨
        Returns:
            ToolMessageç»“æœåˆ—è¡¨
        """

        async def _invoke_tool(tool_call: Dict) -> ToolMessage:
            """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨
            Args:
                tool_call: å·¥å…·è°ƒç”¨è¯·æ±‚å­—å…¸ï¼Œéœ€åŒ…å«name/args/idå­—æ®µ
            Returns:
                å°è£…çš„ToolMessage
            Raises:
                KeyError: å·¥å…·æœªæ³¨å†Œæ—¶æŠ›å‡º
                RuntimeError: å·¥å…·è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
            """
            try:
                # 3. å¼‚æ­¥è°ƒç”¨å·¥å…·
                tool = self.tools_by_name.get(tool_call["name"])  # éªŒè¯ å·¥å…·æ˜¯å¦åœ¨ä¹‹å‰çš„ å·¥å…·é›†åˆä¸­
                if not tool:
                    raise KeyError(f"æœªæ³¨å†Œçš„å·¥å…·: {tool_call['name']}")

                if hasattr(tool, 'ainvoke'):  # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥æ–¹æ³•
                    tool_result = await tool.ainvoke(tool_call["args"])
                else:  # åŒæ­¥å·¥å…·é€šè¿‡çº¿ç¨‹æ± è½¬å¼‚æ­¥
                    loop = asyncio.get_running_loop()
                    tool_result = await loop.run_in_executor(
                        None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                        tool.invoke,  # åŒæ­¥è°ƒç”¨æ–¹æ³•
                        tool_call["args"]  # å‚æ•°
                    )

                # 4. æ„é€ ToolMessage
                return ToolMessage(
                    content=json.dumps(tool_result, ensure_ascii=False),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            except Exception as e:
                print(e)
                raise RuntimeError(f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call['name']}") from e

        try:
            # 5. å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            # asyncio.gather() æ˜¯ Python å¼‚æ­¥ç¼–ç¨‹ä¸­ç”¨äºå¹¶å‘è°ƒåº¦å¤šä¸ªåç¨‹çš„æ ¸å¿ƒå‡½æ•°ï¼Œå…¶æ ¸å¿ƒè¡Œä¸ºåŒ…æ‹¬ï¼š
            # å¹¶å‘æ‰§è¡Œï¼šæ‰€æœ‰ä¼ å…¥çš„åç¨‹ä¼šè¢«åŒæ—¶è°ƒåº¦åˆ°äº‹ä»¶å¾ªç¯ä¸­ï¼Œé€šè¿‡éé˜»å¡ I/O å®ç°å¹¶è¡Œå¤„ç†ã€‚
            # ç»“æœæ”¶é›†ï¼šæŒ‰è¾“å…¥é¡ºåºè¿”å›æ‰€æœ‰åç¨‹çš„ç»“æœï¼ˆæˆ–å¼‚å¸¸ï¼‰ï¼Œä¸ä»»åŠ¡å®Œæˆé¡ºåºæ— å…³ã€‚
            # å¼‚å¸¸å¤„ç†ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œä»»ä¸€ä»»åŠ¡å¤±è´¥ä¼šç«‹å³å–æ¶ˆå…¶ä»–ä»»åŠ¡å¹¶æŠ›å‡ºå¼‚å¸¸ï¼›è‹¥è®¾ç½® return_exceptions=Trueï¼Œåˆ™å¼‚å¸¸ä¼šä½œä¸ºç»“æœè¿”å›ã€‚
            #
            return await asyncio.gather(*[_invoke_tool(tool_call) for tool_call in tool_calls])
        except Exception as e:
            print(e)
            raise RuntimeError("å¹¶å‘æ‰§è¡Œå·¥å…·æ—¶å‘ç”Ÿé”™è¯¯") from e


class State(MessagesState):
    pass


def route_tools_func(state: State):
    """
    åŠ¨æ€è·¯ç”±å‡½æ•°ï¼Œå¦‚æœä»å¤§æ¨¡å‹è¾“å‡ºåçš„AIMessageï¼Œä¸­åŒ…å«æœ‰å·¥å…·è°ƒç”¨çš„è¯·æ±‚(æŒ‡ä»¤)ï¼Œ å°±è¿›å…¥åˆ°toolsèŠ‚ç‚¹ï¼Œ å¦åˆ™åˆ™ç»“æŸ
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END


async def create_graph():
    tools = await mcp_client.get_tools()  # 30ä¸ªä»¥ä¸Šçš„å·¥å…·ï¼Œå…¨éƒ¨æ¥è‡ªMCPæœåŠ¡ç«¯

    builder = StateGraph(State)

    llm_with_tools = llm.bind_tools(tools)

    async def chatbot(state: State):
        return {'messages': [await llm_with_tools.ainvoke(state["messages"])]}

    builder.add_node('chatbot', chatbot)

    tool_node = BasicToolsNode(tools)
    builder.add_node('tools', tool_node)

    builder.add_conditional_edges(
        "chatbot",
        route_tools_func,
        {"tools": "tools", END: END}
    )
    builder.add_edge('tools', 'chatbot')
    builder.add_edge(START, 'chatbot')
    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    return graph


graph = asyncio.run(create_graph())
 # é…ç½®å‚æ•°ï¼ŒåŒ…å«ä¹˜å®¢IDå’Œçº¿ç¨‹ID
config = {
    "configurable": {
        # æ£€æŸ¥ç‚¹ç”±session_idè®¿é—®
        "thread_id": 'zs12311',
    }
}

def add_message(chat_history, user_message):
    if user_message:
        chat_history.append({"role": "user", "content": user_message})
    return chat_history, gr.Textbox(value=None, interactive=False)


async def submit_messages(chat_history: List[Dict]):
    """æµå¼å¤„ç†æ¶ˆæ¯çš„æ ¸å¿ƒå‡½æ•°"""
    user_input = chat_history[-1]['content']
    current_state = graph.get_state(config)
    full_response = ""  # ç´¯ç§¯å®Œæ•´å“åº”
    tool_calls = []  # è®°å½•å·¥å…·è°ƒç”¨

    # å¤„ç†ä¸­æ–­æ¢å¤æˆ–æ­£å¸¸æ¶ˆæ¯
    inputs = Command(resume={'answer': user_input}) if current_state.next else {
        'messages': [HumanMessage(content=user_input)]}

    async for chunk in graph.astream(
            inputs,
            config,
            stream_mode=["messages", "updates"],  # åŒæ—¶ç›‘å¬æ¶ˆæ¯å’ŒçŠ¶æ€æ›´æ–°
    ):
        if 'messages' in chunk:
            for message in chunk[1]:
                # å¤„ç†AIæ¶ˆæ¯æµå¼è¾“å‡º
                if isinstance(message, AIMessage) and message.content:
                    full_response += message.content
                    # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯è€Œéè¿½åŠ 
                    if chat_history and isinstance(chat_history[-1], ChatMessage) and 'title' not in chat_history[-1].metadata:
                        chat_history[-1].content = full_response
                    else:
                        chat_history.append(ChatMessage(role="assistant", content=message.content))
                    yield chat_history

                # å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯
                elif isinstance(message, ToolMessage):
                    tool_msg = f"ğŸ”§ è°ƒç”¨å·¥å…·: {message.name}\n{message.content}"
                    chat_history.append(ChatMessage(role="assistant", content=tool_msg,
                                        metadata={"title": f"ğŸ› ï¸ Used tool {message.name}"}))
                    yield chat_history

    # æ£€æŸ¥æ–°ä¸­æ–­
    current_state = graph.get_state(config)
    if current_state.next:
        interrupt_msg = current_state.interrupts[0].value
        # chat_history.append({'role': 'assistant', 'content': interrupt_msg})
        chat_history.append(ChatMessage(role="assistant", content=interrupt_msg))
        yield chat_history


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(
        title='æˆ‘çš„æ™ºèƒ½å°ç§˜ä¹¦',
        theme=gr.themes.Soft(),
        css=".system {color: #666; font-style: italic;}"  # è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯æ ·å¼
) as demo:
    # èŠå¤©å†å²è®°å½•ç»„ä»¶
    chatbot = gr.Chatbot(
        type="messages",
        height=500,
        render_markdown=True,  # æ”¯æŒMarkdownæ ¼å¼
        line_breaks=False  # ç¦ç”¨è‡ªåŠ¨æ¢è¡Œç¬¦
    )

    # è¾“å…¥ç»„ä»¶
    chat_input = gr.Textbox(
        placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
        label="ç”¨æˆ·è¾“å…¥",
        max_lines=5,
        container=False
    )

    # æ§åˆ¶æŒ‰é’®
    with gr.Row():
        submit_btn = gr.Button("å‘é€", variant="primary")
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")

    # æ¶ˆæ¯æäº¤å¤„ç†é“¾
    msg_handler = chat_input.submit(
        fn=add_message,
        inputs=[chatbot, chat_input],
        outputs=[chatbot, chat_input],
        queue=False
    ).then(
        fn=submit_messages,
        inputs=chatbot,
        outputs=chatbot,
        api_name="chat_stream"  # APIç«¯ç‚¹åç§°
    )

    # æŒ‰é’®ç‚¹å‡»å¤„ç†é“¾
    btn_handler = submit_btn.click(
        fn=add_message,
        inputs=[chatbot, chat_input],
        outputs=[chatbot, chat_input],
        queue=False
    ).then(
        fn=submit_messages,
        inputs=chatbot,
        outputs=chatbot
    )

    # æ¸…ç©ºå¯¹è¯
    clear_btn.click(
        fn=lambda: [],
        inputs=None,
        outputs=chatbot,
        queue=False
    )

    # é‡ç½®è¾“å…¥æ¡†çŠ¶æ€
    msg_handler.then(
        lambda: gr.Textbox(interactive=True),
        None,
        [chat_input]
    )
    btn_handler.then(
        lambda: gr.Textbox(interactive=True),
        None,
        [chat_input]
    )

if __name__ == '__main__':
    demo.launch()